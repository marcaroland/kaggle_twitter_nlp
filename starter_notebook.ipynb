{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_nlp\n",
    "import string\n",
    "import keras\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_cleaning(dataframe):\n",
    "    dataframe['text_cleaned'] = dataframe['text'].str.lower()\n",
    "    dataframe['text_cleaned'] = dataframe['text_cleaned'].apply(lambda x: re.sub(r'http\\S+', '', x))  # Remove URLs\n",
    "    dataframe['text_cleaned'] = dataframe['text_cleaned'].apply(lambda x: re.sub(r'\\d+', '', x))  # Remove numbers\n",
    "    dataframe['text_cleaned'] = dataframe['text_cleaned'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))  # Remove punctuation\n",
    "    dataframe['text_cleaned'] = dataframe['text_cleaned'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))  # Lemmatization & stopword removal\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "train = text_cleaning(train)\n",
    "test = text_cleaning(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenizing -> Assigning a number to each word\n",
    "# <OOV> -> Out of Vocabulary will be used for words which are not known\n",
    "training_sentences = train['text_cleaned']\n",
    "training_labels = train['target']\n",
    "\n",
    "testing_sentences = test['text_cleaned']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Sequencing and padding\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, activation='tanh')),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False, activation='tanh')),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0005),\n",
    "              metrics=[tf.keras.metrics.F1Score(average='binary')])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, verbose=2, batch_size=64)\n",
    "\n",
    "predictions = model.predict(testing_padded)\n",
    "prediction_rounded = [int(pred.round()) for pred in predictions]\n",
    "test['target'] = prediction_rounded\n",
    "test[['id', 'target']].to_csv(\"nlp_submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "                                                                   sequence_length=160,\n",
    "                                                                   name=\"preprocessor_4_tweets\"\n",
    "                                                                  )\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor, \n",
    "                                                               num_classes=2)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model\n",
    "preset = \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(\n",
    "    preset,\n",
    "    sequence_length=160,\n",
    "    name=\"preprocessor_4_tweets\"\n",
    ")\n",
    "\n",
    "# Pretrained classifier\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(\n",
    "    preset,\n",
    "    preprocessor=preprocessor, \n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "classifier.summary()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "training_sentences = train['text_cleaned'].tolist()\n",
    "training_labels = train['target'].astype(int).tolist()\n",
    "\n",
    "# Convert labels to Tensor (important for SparseCategoricalCrossentropy)\n",
    "training_labels = tf.convert_to_tensor(training_labels, dtype=tf.int32)\n",
    "\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics=[\"accuracy\"]  \n",
    ")\n",
    "\n",
    "# Fit with raw text (DistilBERT handles tokenization internally)\n",
    "history = classifier.fit(\n",
    "    x=training_sentences,  # raw text\n",
    "    y=training_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_padded)\n",
    "prediction_rounded = [int(pred.round()) for pred in predictions]\n",
    "test['target'] = prediction_rounded\n",
    "test[['id', 'target']].to_csv(\"nlp_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
